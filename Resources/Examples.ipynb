{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yachana/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yachana/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nepali_stopwords = set(stopwords.words(\"nepali\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenizer(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "def remove_special_char(text):\n",
    "    text = re.sub('[ред,?!]', \"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, stop_words=nepali_stopwords):\n",
    "    filtered_text = [word for word in word_tokenizer(text) if word not in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "def Devnagari_Extractor(text):\n",
    "    text = remove_special_char(text)\n",
    "    text = remove_stopwords(text)\n",
    "    tokens = word_tokenizer(text)\n",
    "\n",
    "    devanagari_range = r'[\\u0900-\\u097F\\\\]'\n",
    "\n",
    "    def is_devanagari(token):\n",
    "        devanagari_chars = [char for char in token if re.match(devanagari_range, char)]\n",
    "        return len(devanagari_chars) >= len(token) / 2\n",
    "\n",
    "    return \" \".join([token for token in tokens if is_devanagari(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"рдХрд╛рдардорд╛рдбреМрдБрднрд┐рддреНрд░ 3%%% рджрд┐рдирднрд░ рдкрд╛рдиреА рдкрд░реНрдиреЗ рд░ рдкреЛрдЦрд░рд╛рд▓рдЧрд╛рдПрддрдорд╛ рдмреБрдЯрд╡рд▓рдорд╛ @ рдкрд╛рдиреА рдирдкрд░реНрдиреЗ Rajan ЁЯТкЁЯТк ЁЯШв реирек резрекрезрез резреирейрезреирейреирезрей\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['рдХрд╛рдардорд╛рдбреМрдБрднрд┐рддреНрд░',\n",
       " '3%%%',\n",
       " 'рджрд┐рдирднрд░',\n",
       " 'рдкрд╛рдиреА',\n",
       " 'рдкрд░реНрдиреЗ',\n",
       " 'рд░',\n",
       " 'рдкреЛрдЦрд░рд╛рд▓рдЧрд╛рдПрддрдорд╛',\n",
       " 'рдмреБрдЯрд╡рд▓рдорд╛',\n",
       " '@',\n",
       " 'рдкрд╛рдиреА',\n",
       " 'рдирдкрд░реНрдиреЗ',\n",
       " 'Rajan',\n",
       " 'ЁЯТкЁЯТк',\n",
       " 'ЁЯШв',\n",
       " 'реирек',\n",
       " 'резрекрезрез',\n",
       " 'резреирейрезреирейреирезрей']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenizer(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'рдХрд╛рдардорд╛рдбреМрдБрднрд┐рддреНрд░ 3%%% рджрд┐рдирднрд░ рдкрд╛рдиреА рдкрд░реНрдиреЗ рд░ рдкреЛрдЦрд░рд╛рд▓рдЧрд╛рдПрддрдорд╛ рдмреБрдЯрд╡рд▓рдорд╛ @ рдкрд╛рдиреА рдирдкрд░реНрдиреЗ Rajan ЁЯТкЁЯТк ЁЯШв реирек резрекрезрез резреирейрезреирейреирезрей'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_special_char(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'рдХрд╛рдардорд╛рдбреМрдБрднрд┐рддреНрд░ 3%%% рджрд┐рдирднрд░ рдкрд╛рдиреА рдкрд░реНрдиреЗ рдкреЛрдЦрд░рд╛рд▓рдЧрд╛рдПрддрдорд╛ рдмреБрдЯрд╡рд▓рдорд╛ @ рдкрд╛рдиреА рдирдкрд░реНрдиреЗ Rajan ЁЯТкЁЯТк ЁЯШв реирек резрекрезрез резреирейрезреирейреирезрей'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'рдХрд╛рдардорд╛рдбреМрдБрднрд┐рддреНрд░ рджрд┐рдирднрд░ рдкрд╛рдиреА рдкрд░реНрдиреЗ рдкреЛрдЦрд░рд╛рд▓рдЧрд╛рдПрддрдорд╛ рдмреБрдЯрд╡рд▓рдорд╛ рдкрд╛рдиреА рдирдкрд░реНрдиреЗ реирек резрекрезрез резреирейрезреирейреирезрей'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = Devnagari_Extractor(sample_text)\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_path = \"/home/yachana/Downloads/suffix.txt\"\n",
    "def get_suffix():\n",
    "    # Create a dictionary based on the length of suffix\n",
    "    with open(suffix_path, 'r') as suff_file:\n",
    "        suffixes = {}\n",
    "        for row in suff_file.read().splitlines():\n",
    "            stem_len = len(list(row))\n",
    "            if stem_len not in suffixes:\n",
    "                suffixes[stem_len] = [row]\n",
    "            else:\n",
    "                suffixes[stem_len] += ([row])\n",
    "\n",
    "    return suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = get_suffix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_suffix(word,suffix):\n",
    "    for L in 9, 8, 7, 6, 5, 4, 3, 2:\n",
    "        if len(word) > L + 1:\n",
    "            for suf in suffix[L]:\n",
    "                if word.endswith(suf):\n",
    "                    ans = word[:-L]\n",
    "                    return ans\n",
    "    return word  # return the original word if no suffix is found\n",
    "\n",
    "def process_sentence(sentence, suffix):\n",
    "    words = sentence.split()\n",
    "    processed_words = [remove_suffix(word, suffix) for word in words]\n",
    "    processed_sentence = ' '.join(processed_words)\n",
    "    return processed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'рдХрд╛рдардорд╛рдбреМрдБ рджрд┐рди рдкрд╛рдиреА рдкрд░реНрдиреЗ рдкреЛрдЦрд░рд╛ рдмреБрдЯрд╡рд▓ рдкрд╛рдиреА рдирдкрд░реНрдиреЗ реирек резрекрезрез резреирейрезреирейреирезрей'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_sentence(sample_text,suffix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a vocabulary of nepali words\n",
    "def build_vocab(data):\n",
    "  vocab = []\n",
    "  for sentence in data:\n",
    "    # Cleaning\n",
    "    cleaned_words = Devnagari_Extractor(sentence)\n",
    "    # Tokenizing\n",
    "    word_tokens = word_tokenizer(cleaned_words)\n",
    "    vocab.extend(word_tokens)\n",
    "  return list(set(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"рдирдорд╕реНрдХрд╛рд░! рдХреЗ рдЫ рдЦрдмрд░?\",\"рд░рд┐рддрд╛ рддрдкрд╛рдИрдВрдХреЛ рдорд┐рддреНрд░ рд╣реЛ\",\"рд░рд╛рдорд▓реЗ рд╡рд╛рдд рдЦрд╛рдпреЛред\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['рдЦрд╛рдпреЛ', 'рд░рд╛рдорд▓реЗ', 'рд╡рд╛рдд', 'рддрдкрд╛рдИрдВрдХреЛ', 'рдорд┐рддреНрд░', 'рд░рд┐рддрд╛', 'рдЦрдмрд░', 'рдирдорд╕реНрдХрд╛рд░']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word:i for i,word in enumerate(vocab)}\n",
    "idx2word = {i:word for word,i in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'рдЦрд╛рдпреЛ': 0,\n",
       " 'рд░рд╛рдорд▓реЗ': 1,\n",
       " 'рд╡рд╛рдд': 2,\n",
       " 'рддрдкрд╛рдИрдВрдХреЛ': 3,\n",
       " 'рдорд┐рддреНрд░': 4,\n",
       " 'рд░рд┐рддрд╛': 5,\n",
       " 'рдЦрдмрд░': 6,\n",
       " 'рдирдорд╕реНрдХрд╛рд░': 7}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'рдЦрд╛рдпреЛ',\n",
       " 1: 'рд░рд╛рдорд▓реЗ',\n",
       " 2: 'рд╡рд╛рдд',\n",
       " 3: 'рддрдкрд╛рдИрдВрдХреЛ',\n",
       " 4: 'рдорд┐рддреНрд░',\n",
       " 5: 'рд░рд┐рддрд╛',\n",
       " 6: 'рдЦрдмрд░',\n",
       " 7: 'рдирдорд╕реНрдХрд╛рд░'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(sent):\n",
    "  #Cleaning\n",
    "  cleaned_words = Devnagari_Extractor(sent)\n",
    "  #Tokenizing\n",
    "  processed_texts = word_tokenizer(cleaned_words)\n",
    "  vector = np.zeros(len(vocab))\n",
    "  for word in processed_texts:\n",
    "    vector[word2idx[word]] = 1\n",
    "  return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1., 1.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words(data[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of times each word appear in documents\n",
    "def count_dict(sentences):\n",
    "    count_dict = {}\n",
    "    for word in vocab:\n",
    "        count_dict[word] = 0\n",
    "    for sent in sentences:\n",
    "        #Cleaning the text\n",
    "        cleaned_words = Devnagari_Extractor(sent)\n",
    "        #Tokenizing the text\n",
    "        processed_texts = word_tokenizer(cleaned_words)\n",
    "        for word in processed_texts:\n",
    "            count_dict[word] += 1\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'рдЦрд╛рдпреЛ': 1, 'рд░рд╛рдорд▓реЗ': 1, 'рд╡рд╛рдд': 1, 'рддрдкрд╛рдИрдВрдХреЛ': 1, 'рдорд┐рддреНрд░': 1, 'рд░рд┐рддрд╛': 1, 'рдЦрдмрд░': 1, 'рдирдорд╕реНрдХрд╛рд░': 1}\n"
     ]
    }
   ],
   "source": [
    "word_count = count_dict(data)\n",
    "print(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(document, word):\n",
    "    N = len(document)\n",
    "    occurance = 0\n",
    "    for token in document:\n",
    "        if token == word:\n",
    "            occurance += 1\n",
    "    return occurance / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_document_frequency(word):\n",
    "    try:\n",
    "        word_occurance = word_count[word] + 1\n",
    "    except:\n",
    "        word_occurance = 1\n",
    "    return np.log(len(data) / word_occurance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(sentence):\n",
    "    vec = np.zeros((len(vocab),))\n",
    "    cleaned_words = Devnagari_Extractor(sentence)\n",
    "    #Tokenizing the text\n",
    "    processed_texts = word_tokenizer(cleaned_words)\n",
    "    for word in processed_texts:\n",
    "        tf = term_frequency(processed_texts, word)\n",
    "        idf = inverse_document_frequency(word)\n",
    "        vec[word2idx[word]] = tf * idf\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['рдирдорд╕реНрдХрд╛рд░! рдХреЗ рдЫ рдЦрдмрд░?', 'рд░рд┐рддрд╛ рддрдкрд╛рдИрдВрдХреЛ рдорд┐рддреНрд░ рд╣реЛ', 'рд░рд╛рдорд▓реЗ рд╡рд╛рдд рдЦрд╛рдпреЛред']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def cal_cosine_similarity(text1, text2):\n",
    "    vec1 = tf_idf(text1)\n",
    "    vec2 = tf_idf(text2)\n",
    "    vec1 = np.asarray(vec1).reshape(-1,vec1.shape[0])\n",
    "    vec2 =  np.asarray(vec2).reshape(-1,vec2.shape[0])\n",
    "    print(cosine_similarity(vec1, vec2))\n",
    "    return cosine_similarity(vec1, vec1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_cosine_similarity(data[0],data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot(y):\n",
    "    n_classes = np.unique(y)\n",
    "    one_hot = np.zeros((len(y), len(n_classes)))\n",
    "    for i, c in enumerate(y):\n",
    "        one_hot[i, n_classes == c] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Assuming you have four classes represented by numbers 0 to 3\n",
    "y = np.array([0, 1, 2, 3, 0, 2, 1, 3, 0])\n",
    "\n",
    "# Convert classes to one-hot encoding\n",
    "one_hot_y = one_hot(y)\n",
    "\n",
    "print(one_hot_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057 0.24472847 0.66524096]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def softmax(z):\n",
    "    exps = np.exp(z)\n",
    "    sum_exps = np.sum(exps)\n",
    "    return exps / sum_exps\n",
    "\n",
    "z = np.array([1.0, 2.0, 3.0])\n",
    "probabilities = softmax(z)\n",
    "\n",
    "print(probabilities)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logestic Regression Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.00001, n_iter=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "        self.w = None\n",
    "\n",
    "    @staticmethod\n",
    "    def one_hot(y):\n",
    "        n_classes = np.unique(y)\n",
    "        one_hot = np.zeros((len(y), len(n_classes)))\n",
    "        for i, c in enumerate(y):\n",
    "            one_hot[i, n_classes == c] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def probabilities(self, X):\n",
    "        z = np.dot(X, self.w.T)\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        return np.argmax(self.probabilities(X), axis=1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.insert(X, 0, 1, axis=1) # (samples,dim) -> (samples, dim + 1)\n",
    "        self.w = np.zeros((len(np.unique(y)), X.shape[1])) # (4, 301)\n",
    "        y = self.one_hot(y)\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            predictions = self.probabilities(X)\n",
    "            error = predictions - y\n",
    "            gradient = np.dot(error.T, X)\n",
    "            self.w -= self.lr * gradient\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating random data\n",
    "np.random.seed(0) # For consistent results\n",
    "X = np.random.rand(100, 300) # 100 samples, 300 features each\n",
    "y = np.random.choice([0, 1, 2, 3], 100) # 100 targets belonging to one of four classes\n",
    "\n",
    "\n",
    "# Creating an instance of the Logistic Regression class\n",
    "model = LogisticRegression(lr=0.00001, n_iter=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after adding bias: (100, 301)\n",
      "Initial shape of weights: (4, 301)\n",
      "Shape of y after one-hot encoding: (100, 4)\n",
      "Shape of predictions: (100, 4)\n",
      "Shape of error: (100, 4)\n",
      "Shape of gradient: (4, 301)\n",
      "Shape of weights after update: (4, 301)\n"
     ]
    }
   ],
   "source": [
    "# Adding bias to the feature matrix\n",
    "X = np.insert(X, 0, 1, axis=1)\n",
    "print(f'Shape of X after adding bias: {X.shape}')  # (100, 301)\n",
    "\n",
    "# Initializing weights\n",
    "model.w = np.zeros((len(np.unique(y)), X.shape[1]))\n",
    "print(f'Initial shape of weights: {model.w.shape}')  # (4, 301)\n",
    "\n",
    "# One-hot encoding the target variable\n",
    "y = model.one_hot(y)\n",
    "print(f'Shape of y after one-hot encoding: {y.shape}')  # (100, 4)\n",
    "\n",
    "for _ in range(model.n_iter):\n",
    "    # Compute probabilities\n",
    "    predictions = model.probabilities(X)\n",
    "    print(f'Shape of predictions: {predictions.shape}')  # (100, 4)\n",
    "\n",
    "    # Compute error\n",
    "    error = predictions - y\n",
    "    print(f'Shape of error: {error.shape}')  # (100, 4)\n",
    "\n",
    "    # Compute gradient\n",
    "    gradient = np.dot(error.T, X)\n",
    "    print(f'Shape of gradient: {gradient.shape}')  # (4, 301)\n",
    "\n",
    "    # Update weights\n",
    "    model.w -= model.lr * gradient\n",
    "    print(f'Shape of weights after update: {model.w.shape}')  # (4, 301)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Category': ['Fruit', 'Vegetable', 'Fruit', 'Vegetable', 'Fruit'],\n",
    "    'Item': ['Apple', 'Carrot', 'Banana', 'Broccoli', 'Apple'],\n",
    "    'Sales': [100, 200, 150, 300, 50]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Group by the 'Category' column and sum the 'Sales' column\n",
    "grouped_data = df.groupby('Category')['Sales'].sum().reset_index()\n",
    "\n",
    "print(grouped_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
